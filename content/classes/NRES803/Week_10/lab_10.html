---
title: "Week 10 Lab"
author: "Drew Tyre"
date: "2019-02-09"
output: html_document
weight: 803101
---



<div id="sleep-study" class="section level1">
<h1>Sleep Study</h1>
<p>This is a great dataset to play with. From the help documentation: The average reaction time per day for subjects in a sleep deprivation study. On day 0 the subjects had their normal amount of sleep. Starting that night they were restricted to 3 hours of sleep per night. The observations represent the average reaction time on a series of tests given each day to each subject.</p>
<pre class="r"><code>library(NRES803)
library(tidyverse)
library(lme4)  ## notice conflicts with tidyr
summary(ss.lm &lt;- lm(Reaction ~ Days, sleepstudy))</code></pre>
<p>As you might expect, the longer one goes without sleep, the slower one’s reaction time gets. There is alot of variation in the relationship.</p>
<pre class="r"><code>ssBase &lt;- ggplot(sleepstudy, aes(x = Days, y = Reaction)) + geom_point(aes(color = Subject)) + 
    scale_color_discrete() + labs(x = &quot;Days with no sleep&quot;, y = &quot;Reaction time [ms]&quot;)
ssBase + geom_smooth(method = &quot;lm&quot;)</code></pre>
<p>However, if the relationship is plotted seperately for each individual in the dataset, then the signal gets much stronger. Clearly people differ in their responses to sleep deprivation. One subject got faster as they missed out on sleep!</p>
<pre class="r"><code># group by subject
ssBase + geom_smooth(method = &quot;lm&quot;) + facet_wrap(~Subject) + 
    guides(color = FALSE)
# suppress legend because facet labels do the job</code></pre>
<p>We could fit a seperate model to each subject and combine them. This is effectively a Subject*Days interaction model.</p>
<pre class="r"><code>fm1 &lt;- lmList(Reaction ~ Days | Subject, sleepstudy)
cc &lt;- coef(fm1)
cc</code></pre>
<p>The <code>lmList()</code> function fits a different linear model to each subset of a data.frame defined by the variable after the “|”, in this case “Subject”.</p>
<ol class="example" style="list-style-type: decimal">
<li>Calculate the mean intercept and Days coefficients from the matrix created by <code>coef(fm1)</code>. Compare these with the estimates from the single regression model. Use <code>summary(fm1)</code> to see the residual variance and degrees of freedom for the pooled list. Compare these values with the residual variance and degrees of freedom from the single regression model.</li>
</ol>
<p>We can use a mixed model approach to get the best of both worlds - an estimate of the population average intercept and slope, and an idea of how much variation there is among individuals. The function lmer() in package lme4 is the method of choice here. The fixed effects part of the formula is identical to the lm() formula, with a response variable on the left of the ˜ and a covariate on the right. The new part specifies the random effects, and these are given in (). To the left of the | are the fixed effects that will vary among groups, and to the right of the | are the variable(s) that specify the groups. In this case the variable Subject defines the groups. As with other formulas, the intercept is implicit, so (Days | Subject) indicates that both the intercept and the slope are allowed to vary among subjects. This is the same as fitting a seperate line to each subject, as we did above.</p>
<pre class="r"><code># fit a mixed model
summary(ss.mm &lt;- lmer(Reaction ~ Days + (Days | Subject), sleepstudy, 
    REML = FALSE))</code></pre>
<p>Looking at the summary of the mixed model, you should see a “fixed effects” section which is identical to that from a normal lm() fit, with one notable exception. There are no p-values reported for each coefficient. This isn’t a mistake, it is a deliberate choice by the software designer to avoid an argument over the correct number of degrees of freedom to use for the test!</p>
<ol start="2" class="example" style="list-style-type: decimal">
<li>Compare the estimates and standard errors with those from the pooled data model.</li>
</ol>
<p>The new bit is the section labelled “Random effects”. We can see that the model has estimated not just one variance, but three, plus a correlation coefficient. One variance estimate is the residual variance, with the same interpretation as always. The other two variances indicate the variance of the “random effects” on the intercept and the slope. These random perturbations to the population are assumed to be correlated to some extent, so there
is also a correlation parameter. In this case the random effects are relatively uncorrelated.</p>
<p>We can also get the random perturbations for each group, and combining those with the population level coefficients gives us the coefficients for the line in each group, as shown in the following table. When the random effects (shown in the left 2 columns) are negative, the coefficients for the group are less than the population coefficients, and when they are positive, the group coefficients are greater.</p>
<p>In the following figure I’ve plotted the intercepts for each group from the mixed model against the intercepts from the list of models. If they were identical they would fall on the solid line.</p>
<pre class="r"><code>shrink &lt;- data.frame(Random = coef(ss.mm)$Subject, Fixed = cc)
t1 &lt;- shrink %&gt;% gather(coefficient, estimate) %&gt;% separate(coefficient, 
    c(&quot;model&quot;, &quot;coefficient&quot;), extra = &quot;drop&quot;) %&gt;% group_by(model) %&gt;% 
    mutate(id = row_number()) %&gt;% spread(model, estimate)

# phew! that was more work than I expected.

# make a little dataframe for the average point
meanpoint &lt;- data.frame(coefficient = c(&quot;Intercept&quot;, &quot;Days&quot;), 
    Random = fixef(ss.mm), Fixed = coef(ss.lm))
# now make the plot ... easier b/c of data manipulation?

ggplot(t1, aes(x = Fixed, y = Random)) + geom_point() + facet_wrap(~coefficient, 
    scales = &quot;free&quot;) + geom_smooth(method = &quot;lm&quot;) + geom_abline(slope = 1, 
    intercept = 0, linetype = 2) + geom_point(data = meanpoint, 
    color = &quot;violetred1&quot;, size = 2)</code></pre>
<p>Random effects coefficients are slightly biased towards the mean value indicated by the red dot. Therefore the blue line (fit to the points) has a slope less than 1. The dashed line has a slope of 1 and an intercept of 0. This phenomenon
is called “shrinkage”, because the estimates of the within group coefficients “shrink” towards the population mean.</p>
</div>
<div id="mouse-example" class="section level1">
<h1>Mouse Example</h1>
<p>Let’s look at January Frost’s mouse dataset. These are morphological measurements of mice caught at various sites in the Niobrara Valley. Let’s look at the breakdown of the relationship between ear size and foot length by geographic site. Sex also matters.</p>
<pre class="r"><code>data(mice)
mice &lt;- filter(mice, sex != &quot;U&quot;)
basemouse &lt;- ggplot(mice, aes(x = foot, y = ear)) + geom_point(alpha = 0.2) + 
    xlab(&quot;Foot Length [mm]&quot;) + ylab(&quot;Ear Length [mm]&quot;)

basemouse + geom_smooth(method = &quot;lm&quot;) + facet_wrap(~site)</code></pre>
<p>A few things are obvious from this plot. First, January collected alot of data at a few sites, and much less at others. Some of the sex size effects are clear in a few sites (two groups of points), but much less clear in others. But most interesting is the variation in slope. In many sites the bigger the feet the bigger the ears. But not everywhere! Put it all in one plot, adjusting the alpha level so overplotting is clearer.</p>
<pre class="r"><code>basemouse + geom_smooth(aes(group = site), method = &quot;lm&quot;, se = FALSE)</code></pre>
<p>So we want to check a model that lets the relationship between feet and ears vary by species and sex. The sites are a random sample of possible places we could look for mice, so we’ll treat site as a random effect. We can also see that the slope of the effect varies alot between sites, so we’ll let the coefficient of foot vary between sites too, at least initially. Here’s the global model, and check the residuals:</p>
<pre class="r"><code>mice.global &lt;- lmer(ear ~ foot * species * sex + (foot | site), 
    data = mice)

rr &lt;- broom::augment(mice.global)
ggplot(rr, aes(x = .fitted, y = .resid)) + geom_point() + geom_smooth() + 
    geom_hline(yintercept = 0, linetype = 2)

# get int and slope for qqline
probs &lt;- c(0.25, 0.75)
y &lt;- quantile(rr$.resid, probs, names = FALSE, na.rm = TRUE)
x &lt;- qnorm(probs)
slope &lt;- diff(y)/diff(x)
int &lt;- y[1L] - slope * x[1L]
ggplot(rr, aes(sample = .resid)) + geom_qq(size = rel(4)) + geom_abline(intercept = int, 
    slope = slope, linetype = 2, size = 2)

ggplot(rr, aes(x = .fitted, y = sqrt(abs(.resid)))) + geom_point() + 
    geom_smooth() + geom_hline(yintercept = 1)</code></pre>
<p>The residuals don’t look too bad, although there are some large positive residuals which we’ll ignore for now. So let’s fit a range of models with different random effects structures, and compare using AIC. The problem with comparing different random effects using Likelihood Ratio tests is that our null hypothesis is “on the boundary” of the parameter space, because you can’t have negative variances. Choosing a model that has the minimum AIC won’t be bothered by this consideration, but this would affect the calculation of the weights.</p>
<pre class="r"><code># uncorrelated random effects
mice.1 &lt;- lmer(ear ~ foot * species * sex + (foot - 1 | site) + 
    (1 | site), data = mice)
# only random slope
mice.2 &lt;- lmer(ear ~ foot * species * sex + (foot - 1 | site), 
    data = mice)
# only random intercept
mice.3 &lt;- lmer(ear ~ foot * species * sex + (1 | site), data = mice)

aic &lt;- sapply(list(mice.global, mice.1, mice.2, mice.3), AIC)
knitr::kable(cbind(Model = c(&quot;Correlated&quot;, &quot;Uncorrelated&quot;, &quot;Slope&quot;, 
    &quot;Intercept&quot;), AIC = format(aic, digits = 5)))</code></pre>
<p>We’re in luck, the model with both a random slope and intercept where the effects are correlated has the lowest AIC, and by a large margin. In the next phase of model selection we use package lmerTest to evaluate the fixed effects structures using backwards selection with F tests.</p>
<pre class="r"><code>library(lmerTest)
mice.global &lt;- lmer(ear ~ foot * species * sex + (foot | site), 
    data = mice)
coefTable &lt;- summary(mice.global, ddf = &quot;Kenward-Roger&quot;)$coefficients
knitr::kable(coefTable, digits = 2, caption = &quot;Fixed effects coefficients from a global model of mouse ear length. Degrees of freedom from the Kenward-Roger approximation.&quot;)</code></pre>
<p>Unlike the default <code>lmer()</code>, the extra functions in <code>lmerTest</code> push the p values out following the practices used in SAS. There is also a nice function that does automatic backwards selection. In order to stay within the R framework we have to change the default values to get a Type I sums of squares test.</p>
<pre class="r"><code>step(mice.global, ddf = &quot;Satterthwaite&quot;)</code></pre>
<p>Our final model is simply <code>ear ~ foot + (1+foot|site)</code>. This means the relationship between ear and foot length is constant between sexes and species, but varies geographically. I find that quite fascinating. I’ll fit this final model, and make a plot or two.</p>
<pre class="r"><code>mice.final &lt;- lmer(ear ~ foot + (foot | site), data = mice)
summary(mice.final)  # capture the output</code></pre>
<p>You can make predictions from these models just as you would with other models. To use <code>broom::augment()</code> you have to specify both the fixed effects values <em>and</em> a value for the random effect.</p>
<pre class="r"><code>library(broom)
nd &lt;- data.frame(foot = seq(min(mice$foot), max(mice$foot), length = 50), 
    site = factor(&quot;42&quot;, levels = unique(mice$site)))

pred42 &lt;- augment(mice.final, newdata = nd) %&gt;% rename(ear = .fitted)

ggplot(mice, aes(x = foot, y = ear)) + geom_point(alpha = 0.2) + 
    xlab(&quot;Foot Length [mm]&quot;) + ylab(&quot;Ear Length [mm]&quot;) + geom_point(data = filter(mice, 
    site == &quot;42&quot;), alpha = 1, color = &quot;red&quot;) + geom_line(data = pred42, 
    size = 2, color = &quot;red&quot;)</code></pre>
<p>So you could get lines for every site, but that would be a bit messy. We say these predictions are “conditional” on the random effects, because they include the perturbations from the random effects. Another thing you might want are predictions “unconditional” on the random effects. We can get these too but not from <code>broom::augment()</code>. Package lme4 includes a <code>predict()</code> function for these fitted objects.</p>
<pre class="r"><code># get predictions unconditional on random effect
preduc &lt;- cbind(nd, ear = predict(mice.final, newdata = nd, re.form = ~0))

# make the plot above, but fade out the red line and add
# unconditional prediction
ggplot(mice, aes(x = foot, y = ear)) + geom_point(alpha = 0.2) + 
    xlab(&quot;Foot Length [mm]&quot;) + ylab(&quot;Ear Length [mm]&quot;) + geom_line(data = pred42, 
    size = 2, color = &quot;red&quot;, alpha = 0.4) + geom_line(data = preduc, 
    size = 2)</code></pre>
<p>That line represents the relationship between ears and feet we expect to find at a randomly chosen new site.</p>
<p>What about confidence intervals on our predictions? A very good question, young grasshopper, but one that will have to wait!</p>
</div>
